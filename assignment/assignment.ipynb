{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58ffdee1",
      "metadata": {
        "id": "58ffdee1"
      },
      "source": [
        "<a target=\"_blank\"\n",
        "  href=\"https://colab.research.google.com/github/sk-classroom/word-embedding/blob/main/assignment/assignment.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# Assignment\n",
        "\n",
        "In this assignment, we will create a word embedding using the tf-idf matrix and the word2vec model from the text of Les Mis√©rables.\n",
        "\n",
        "Complete the following tasks and upload your notebook to your GitHub repository.\n",
        "\n",
        "1. Fill in the blank functions, marked by, \"\\#TODO\", in the notebook\n",
        "2. Update this notebook by using `git add`, `git commit`, and then `git push`.\n",
        "3. The notebook will be automatically graded, and your score will be shown on GitHub. See [how to check the results on GitHub](https://docs.github.com/en/education/manage-coursework-with-github-classroom/learn-with-github-classroom/view-autograding-results)\n",
        "\n",
        "Dont's:\n",
        "- Do not import any libraries except for the ones that are already imported. The grading script will not work if you import additional libraries.\n",
        "- Do not change the name of the functions. The grading script will not work if you change the name of the functions.\n",
        "- DO not remove the cells with tag \"gradable\". If you happen to remove the cells with tag \"gradable\", recreate the cell and add the tag \"gradable\" to the cell.\n",
        "- Do not import any libraries that are not listed in the first cell. The grading script will not work if you import additional libraries.\n",
        "\n",
        "Do's:\n",
        "- Make the code clearn and easy to understand. Consider using linters & formatters such as `black`. [Youtube: Linters and fixers: never worry about code formatting again (Vim + Ale + Flake8 & Black for Python) by Yong-Yeol Ahn](https://www.youtube.com/watch?v=4FKPySR6HLk)\n",
        "- You can add additional cells as you like. If you want to include the additional cells in the grading, you need to add the tag \"gradable\" to the cell. Otherwise, your additional cells will be stripped out when grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cbf84dc9",
      "metadata": {
        "id": "cbf84dc9"
      },
      "outputs": [],
      "source": [
        "# For Colab users, uncomment the following line to intall the libraries.\n",
        "# !pip install spacy requests numpy pandas scipy umap-learn scikit-learn\n",
        "\n",
        "# If this is the first time using spacy (a library for natural language processing), uncomment and run the following cell to download the English model.\n",
        "# !python -m spacy downl# For Colab users, uncomment the following line to intall the libraries.\n",
        "# !pip install spacy requests numpy pandas scipy umap-learn scikit-learn\n",
        "\n",
        "# If this is the first time using spacy (a library for natural language processing), uncomment and run the following cell to download the English model.\n",
        "# !python -m spacy download en_core_web_smoad en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "53864f86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "53864f86",
        "outputId": "9e5fa58e-d201-4345-b73c-bd0b494dc715"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d80e6fdaa89d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfo\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplain\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/cli/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_model\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfind_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_function\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfind_threshold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_threshold\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/cli/evaluate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwasabi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_available_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_in_jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependencyRenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEntityRenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpanRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/displacy/render.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminify_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .templates import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mTPL_DEP_ARCS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mTPL_DEP_SVG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_init_module_attrs\u001b[0;34m(spec, module, override)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mcached\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_cached\u001b[0;34m(filename)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# You can use the following libraries. Note that other libraries (and the functions from the libraries not listed here) are not allowed, and grading will fail if you use them.\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import umap\n",
        "from sklearn.decomposition import PCA\n",
        "import spacy\n",
        "from tqdm.auto import tqdm\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For visualization\n",
        "from bokeh.plotting import figure, show, output_notebook\n",
        "from bokeh.models import ColumnDataSource\n",
        "from bokeh.io import push_notebook\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.models import ColumnDataSource, HoverTool\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e667a9",
      "metadata": {
        "id": "86e667a9"
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "Let us first download the text of Les Mis√©rables from Project Gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c0675c",
      "metadata": {
        "id": "58c0675c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Download Les Mis√©rables text from Project Gutenberg\n",
        "url = \"https://www.gutenberg.org/cache/epub/135/pg135.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b80dbb",
      "metadata": {
        "id": "b2b80dbb"
      },
      "source": [
        "Let us then clean the text by lemmatizing the words, removing stop words, and converting to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950fd433",
      "metadata": {
        "id": "950fd433"
      },
      "outputs": [],
      "source": [
        "# Remove the header and footer\n",
        "text = (\n",
        "    \"CHAPTER I‚ÄîM. MYRIEL\"\n",
        "    + text.split(\"CHAPTER I‚ÄîM. MYRIEL\")[-1].split(\n",
        "        \"*** END OF THIS PROJECT GUTENBERG EBOOK LES MIS√âRABLES ***\"\n",
        "    )[0]\n",
        ")\n",
        "\n",
        "# Split text into chapters and process each chapter\n",
        "chapters = text.split(\"CHAPTER\")  # Split by chapter, ignore text before first chapter\n",
        "\n",
        "docs = []\n",
        "for chapter in tqdm(chapters):\n",
        "    nlp.max_length = len(chapter) + 100\n",
        "    doc = nlp(chapter)\n",
        "\n",
        "    # Clean the text:\n",
        "    # 1. Remove stop words\n",
        "    # 2. Remove punctuation\n",
        "    # 3. Lemmatize words\n",
        "    # 4. Convert to lowercase\n",
        "    for sent in doc.sents:\n",
        "        cleaned_tokens = [\n",
        "            token.lemma_.lower()\n",
        "            for token in sent\n",
        "            if not token.is_stop and not token.is_punct and not token.is_space\n",
        "        ]\n",
        "\n",
        "        if len(cleaned_tokens) == 0:\n",
        "            continue\n",
        "\n",
        "        sent = \" \".join(cleaned_tokens)\n",
        "\n",
        "        docs.append(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f2018f",
      "metadata": {
        "id": "66f2018f"
      },
      "source": [
        "The `docs` is a list of strings, and each string is a sentence. For example, the 450th sentence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdaeab7",
      "metadata": {
        "id": "7fdaeab7"
      },
      "outputs": [],
      "source": [
        "print(docs[450])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2373c617",
      "metadata": {
        "id": "2373c617"
      },
      "source": [
        "Notice that the sentence does not look natural due to the clearning process, which is expected. We will use the clearned sentences (`docs`) for the rest of the assignment to generate the character embeddings.\n",
        "\n",
        "## Statistics of the data\n",
        "\n",
        "A common situation in text analysis is heterogeneity in the length of the documents. Let us first check the distribution of the document length to confirm the heterogeneity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c50e74d",
      "metadata": {
        "id": "2c50e74d"
      },
      "outputs": [],
      "source": [
        "# Distribution of document length\n",
        "doc_lengths = [len(doc.split()) for doc in docs]\n",
        "\n",
        "doc_length_count = np.bincount(doc_lengths)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "sns.pointplot(x=np.arange(len(doc_length_count)), y=doc_length_count, ax=ax)\n",
        "\n",
        "ax.set_title(\"Distribution of document length\")\n",
        "ax.set_xlabel(\"Document length\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xscale(\"log\")\n",
        "\n",
        "sns.despine()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340c7562",
      "metadata": {
        "id": "340c7562"
      },
      "source": [
        "Observe that the document length is highly skewed with a long tail. Some documents are very short consiting of only a few words, while others are very long, containing more than 100 words!\n",
        "\n",
        "Another heterogeneity is the frequency of the words. While spacy removes the stop words that appear frequently but are not informative such as \"the\", \"is\", \"in\", etc., some words are still very frequent. Let us check the frequency of the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a51caf9",
      "metadata": {
        "id": "4a51caf9"
      },
      "outputs": [],
      "source": [
        "# Frequency of words\n",
        "from collections import Counter\n",
        "\n",
        "words = \" \".join(docs).split()\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# Get all words that appear more than 10 times\n",
        "df_word_freq = pd.DataFrame(word_freq.items(), columns=[\"word\", \"freq\"])\n",
        "df_word_freq = df_word_freq[df_word_freq[\"freq\"] > 10]\n",
        "df_word_freq = df_word_freq.sort_values(by=\"freq\", ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.pointplot(x=np.arange(1000), y=\"freq\", data=df_word_freq.head(1000), ax=ax)\n",
        "ax.set_title(\"Frequency of words\")\n",
        "ax.set_xlabel(\"Rank of word in descending order of frequency\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xscale(\"log\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "740762c6",
      "metadata": {
        "id": "740762c6"
      },
      "source": [
        "This is also highly skewed with a long tail. In fact, it is known that the frequency of words robsutly follow a power law distribution. See the lecture notes for more details.\n",
        "\n",
        "# Assignment\n",
        "\n",
        "## Task 1: Calculate the tf-idf matrix\n",
        "\n",
        "The tf-idf matrix is a matrix that represents the importance of each word in each document. The tf-idf matrix is calculated by first calculating the term frequency (tf) matrix, then calculating the inverse document frequency (idf) matrix, and then multiplying the two matrices.\n",
        "Your task is to implement the function `calculate_tf_idf_matrix` to calculate the tf-idf matrix. This function should return the tf-idf matrix and the mapping from word to index.\n",
        "1. Input: `docs`: list of strings, each string is a document.\n",
        "2. Output: `tfidf_matrix`: 2D numpy array, the tf-idf matrix of size (n_words, n_docs).\n",
        "3. Output: `word_to_idx`: dictionary, the mapping from word to index with keys as words and values as indices of the tf-idf matrix.\n",
        "\n",
        "Note:\n",
        "- You should split each document into words by space using the `split()` method, e.g., docs[0].split() will return a list of words in the first document.\n",
        "- The values of the word_to_idx should be integers starting from 0. The index corresponds to the row index of the tf-idf matrix. For example, if the word \"javert\" is the 10th word in the tf-idf matrix, the index of \"javert\" in the word_to_idx should be 10.\n",
        "- The process may take a long time if you naively use it using the for loops. Consider this as the opportunity to learn convenient numpy operations such as `np.einsum` and `np.sum` to speed up the computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b8ea62",
      "metadata": {
        "tags": [
          "gradable"
        ],
        "id": "54b8ea62"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement this function\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tf_idf_matrix(docs):\n",
        "    \"\"\"Generate tf-idf matrix from documents.\"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(docs).toarray()\n",
        "    word_to_idx = vectorizer.vocabulary_\n",
        "    return tfidf_matrix, word_to_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9fce0c",
      "metadata": {
        "id": "df9fce0c"
      },
      "outputs": [],
      "source": [
        "tfidf_matrix, word_to_idx = calculate_tf_idf_matrix(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14fccf0",
      "metadata": {
        "id": "f14fccf0"
      },
      "outputs": [],
      "source": [
        "# If you could compute the tf-idf matrix correctly, the following test should pass.\n",
        "data = np.load(\"tfidf_matrix_signature.npz\")\n",
        "u, s, vh = data[\"u\"], data[\"s\"], data[\"vh\"]\n",
        "assert (\n",
        "    np.mean(np.abs(np.diag(u.T @ tfidf_matrix @ vh.T) - s)) < 1e-2\n",
        "), \"The tfidf matrix is not correctly computed.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb00c121",
      "metadata": {
        "id": "cb00c121"
      },
      "source": [
        "## Task 2: Embed the characters\n",
        "\n",
        "The characters in Les Mis√©rables are grouped into several categories. We will use the tf-idf matrix to embed the characters into a 2D space.\n",
        "\n",
        "Implement the function `embed_tfidf_matrix` to embed the words into a 32 dimensional space using PCA.\n",
        "The matrix will take the tf-idf vectors of the words as input and return the embedded vectors.\n",
        "The embedding will be generated by using the PCA.\n",
        "The input and output are as follows:\n",
        "1. Input: `tfidf_matrix`: 2D numpy array, the tf-idf vectors of the words, of size (n_words, n_docs).\n",
        "2. Output: `emb`: 2D numpy array, the embedded vectors of the characters, of size (n_characters, 32).\n",
        "\n",
        "Note:\n",
        "- You can use the `PCA` class from `sklearn.decomposition` to perform the PCA.\n",
        "- The number of components should be 32.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aac4da3",
      "metadata": {
        "tags": [
          "gradable"
        ],
        "id": "3aac4da3"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement this function\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def embedding_characters(character_tfidx_matrix):\n",
        "    \"\"\"Embed characters into 32D space using PCA.\"\"\"\n",
        "    pca = PCA(n_components=32)\n",
        "    embedded_characters = pca.fit_transform(character_tfidx_matrix)\n",
        "    return embedded_characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95307ad",
      "metadata": {
        "id": "b95307ad"
      },
      "outputs": [],
      "source": [
        "emb = embedding_characters(tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01d00a8",
      "metadata": {
        "id": "c01d00a8"
      },
      "outputs": [],
      "source": [
        "# If you could compute the embedding correctly, the following test should pass.\n",
        "assert emb.shape[1] == 32, \"The embedding dimension is not 32\"\n",
        "assert emb.shape[0] == len(word_to_idx), \"The number of words is not correct\"\n",
        "assert np.mean(np.linalg.norm(emb, axis=0)) > 0.99, \"The embedding is not normalized\"\n",
        "assert np.allclose(\n",
        "    np.mean(emb, axis=0), np.zeros(32), atol=1e-2\n",
        "), \"The embedding is not centered\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e311801f",
      "metadata": {
        "id": "e311801f"
      },
      "source": [
        "Let us visualize the character embedding using UMAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7747dea",
      "metadata": {
        "id": "a7747dea"
      },
      "outputs": [],
      "source": [
        "df_characters = pd.DataFrame(\n",
        "    [\n",
        "        [\"myriel\", \"\", \"male\"],\n",
        "        [\"napoleon\", \"\", \"male\"],\n",
        "        [\"cravatte\", \"\", \"male\"],\n",
        "        [\"labarre\", \"\", \"male\"],\n",
        "        [\"jean\", \"valjean_household\", \"male\"],\n",
        "        [\"marguerite\", \"\", \"female\"],\n",
        "        [\"isabeau\", \"\", \"female\"],\n",
        "        [\"gervais\", \"\", \"male\"],\n",
        "        [\"listolier\", \"young_paris_circle\", \"male\"],\n",
        "        [\"fameuil\", \"young_paris_circle\", \"male\"],\n",
        "        [\"favourite\", \"young_paris_circle\", \"female\"],\n",
        "        [\"dahlia\", \"young_paris_circle\", \"female\"],\n",
        "        [\"fantine\", \"young_paris_circle\", \"female\"],\n",
        "        [\"cosette\", \"valjean_household\", \"female\"],\n",
        "        [\"javert\", \"trial_characters\", \"male\"],\n",
        "        [\"fauchelevent\", \"valjean_household\", \"male\"],\n",
        "        [\"bamatabois\", \"\", \"male\"],\n",
        "        [\"simplice\", \"trial_characters\", \"female\"],\n",
        "        [\"scaufflaire\", \"trial_characters\", \"male\"],\n",
        "        [\"judge\", \"trial_characters\", \"male\"],\n",
        "        [\"champmathieu\", \"trial_characters\", \"male\"],\n",
        "        [\"brevet\", \"trial_characters\", \"male\"],\n",
        "        [\"chenildieu\", \"trial_characters\", \"male\"],\n",
        "        [\"cochepaille\", \"trial_characters\", \"male\"],\n",
        "        [\"pontmercy\", \"\", \"male\"],\n",
        "        [\"boulatruelle\", \"\", \"male\"],\n",
        "        [\"gribier\", \"\", \"male\"],\n",
        "        [\"jondrette\", \"\", \"male\"],\n",
        "        [\"gavroche\", \"\", \"male\"],\n",
        "        [\"gillenormand\", \"\", \"male\"],\n",
        "        [\"magnon\", \"\", \"female\"],\n",
        "        [\"marius\", \"\", \"male\"],\n",
        "        [\"mabeuf\", \"\", \"male\"],\n",
        "        [\"enjolras\", \"revolutionary_core\", \"male\"],\n",
        "        [\"combeferre\", \"revolutionary_core\", \"male\"],\n",
        "        [\"prouvaire\", \"revolutionary_core\", \"male\"],\n",
        "        [\"feuilly\", \"revolutionary_core\", \"male\"],\n",
        "        [\"courfeyrac\", \"revolutionary_core\", \"male\"],\n",
        "        [\"bahorel\", \"revolutionary_core\", \"male\"],\n",
        "        [\"bossuet\", \"revolutionary_core\", \"male\"],\n",
        "        [\"joly\", \"revolutionary_core\", \"male\"],\n",
        "        [\"grantaire\", \"revolutionary_core\", \"male\"],\n",
        "        [\"gueulemer\", \"thenardier_gang\", \"male\"],\n",
        "        [\"babet\", \"thenardier_gang\", \"male\"],\n",
        "        [\"claquesous\", \"thenardier_gang\", \"male\"],\n",
        "        [\"montparnasse\", \"thenardier_gang\", \"male\"],\n",
        "        [\"toussaint\", \"valjean_household\", \"female\"],\n",
        "        [\"brujon\", \"thenardier_gang\", \"male\"],\n",
        "    ],\n",
        "    columns=[\"character\", \"group\", \"gender\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7927bb7b",
      "metadata": {
        "id": "7927bb7b"
      },
      "source": [
        "Let us then retrieve the tf-idf vectors for the characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f19a146c",
      "metadata": {
        "id": "f19a146c"
      },
      "outputs": [],
      "source": [
        "les_miserables_characters = df_characters[\"character\"].tolist()\n",
        "\n",
        "emb_characters = np.zeros((len(les_miserables_characters), emb.shape[1]))\n",
        "for i, character in enumerate(les_miserables_characters):\n",
        "    emb_characters[i, :] = emb[word_to_idx[character], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8392920",
      "metadata": {
        "id": "d8392920"
      },
      "outputs": [],
      "source": [
        "# If you could compute the embedding correctly, the following test should pass.\n",
        "# The similarity between the characters within the groups should be higher than the similarity between the characters across the groups.\n",
        "# The similarity is measured by the cosine similarity.\n",
        "focal_groups = [\"revolutionary_core\", \"young_paris_circle\", \"thenardier_gang\"]\n",
        "n_emb_characters = np.einsum(\n",
        "    \"ij,i->ij\", emb_characters, 1.0 / np.linalg.norm(emb_characters, axis=1)\n",
        ")\n",
        "group_centroids = np.array(\n",
        "    [\n",
        "        np.mean(n_emb_characters[df_characters[\"group\"] == group], axis=0)\n",
        "        for group in focal_groups\n",
        "    ]\n",
        ")\n",
        "group_similarieis = group_centroids @ group_centroids.T\n",
        "within_group_similarities = np.mean(np.diag(group_similarieis))\n",
        "between_group_similarities = np.sum(group_similarieis - np.diag(group_similarieis)) / (\n",
        "    len(focal_groups) * (len(focal_groups) - 1)\n",
        ")\n",
        "\n",
        "assert (\n",
        "    within_group_similarities > between_group_similarities\n",
        "), \"The groups are not well separated\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1c8575",
      "metadata": {
        "id": "ba1c8575"
      },
      "source": [
        "Now, let's explore the emebdding space using the `gensim` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347ba353",
      "metadata": {
        "id": "347ba353"
      },
      "outputs": [],
      "source": [
        "# Create a KeyedVectors object from the embedding matrix that allows us to use the gensim's APIs to explore the embedding space.\n",
        "kv = gensim.models.KeyedVectors(emb.shape[1])\n",
        "kv.add_vectors(\n",
        "    [word for word in word_to_idx.keys()], emb\n",
        ")  # This is how you add vectors to the KeyedVectors object"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8983554b",
      "metadata": {
        "id": "8983554b"
      },
      "source": [
        "Let us first see the most similar words to \"javert\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31685fd8",
      "metadata": {
        "id": "31685fd8"
      },
      "outputs": [],
      "source": [
        "kv.most_similar(\"jean\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b50caa",
      "metadata": {
        "id": "95b50caa"
      },
      "source": [
        "You might observe that some characters are very similar to each other. For example, \"valjean\" and \"jean\" are very similar which reflects the fact that Jean Valjean is literally \"Jean\" - they're the same character!\n",
        "Additionally, you might observe that \"mathieu\" and \"valjean\" are also very similar. This is because \"mathieu\" is the character who is mistaken for \"valjean\" in the story."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7633674b",
      "metadata": {
        "id": "7633674b"
      },
      "source": [
        "Finally, let us visualize the embedding using UMAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7e9e04",
      "metadata": {
        "id": "5b7e9e04"
      },
      "outputs": [],
      "source": [
        "output_notebook()\n",
        "\n",
        "# Create a list of colors and groups for each character\n",
        "\n",
        "\n",
        "def visualize_characters(embedded_characters, group_name=\"group\"):\n",
        "    \"\"\"\n",
        "    Visualize the characters using UMAP\n",
        "    \"\"\"\n",
        "    reducer = umap.UMAP(\n",
        "        n_components=2, metric=\"cosine\", n_neighbors=15, min_dist=0.05, random_state=42\n",
        "    )\n",
        "    xy = reducer.fit_transform(embedded_characters)\n",
        "    colors = []\n",
        "    groups = []\n",
        "    palette = sns.color_palette().as_hex()\n",
        "    group_colors = {\n",
        "        group: palette[i] for i, group in enumerate(df_characters[group_name].unique())\n",
        "    }\n",
        "    group_colors[\"\"] = \"#fafafa\"\n",
        "    for idx, row in df_characters.iterrows():\n",
        "        color = group_colors[row[group_name]]\n",
        "        colors.append(color)\n",
        "        groups.append(row[group_name])\n",
        "\n",
        "    # Prepare data for Bokeh\n",
        "    source = ColumnDataSource(\n",
        "        data=dict(\n",
        "            x=xy[:, 0],\n",
        "            y=xy[:, 1],\n",
        "            text_x=xy[:, 0],\n",
        "            text_y=xy[:, 1],\n",
        "            character=les_miserables_characters,\n",
        "            colors=colors,\n",
        "            group=groups,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    p = figure(\n",
        "        title=\"Les Mis√©rables Character embedding\",\n",
        "        x_axis_label=\"X\",\n",
        "        y_axis_label=\"Y\",\n",
        "        width=800,\n",
        "        height=600,\n",
        "    )\n",
        "\n",
        "    # Add hover tool\n",
        "    hover = HoverTool(tooltips=[(\"Character\", \"@character\"), (\"Group\", \"@group\")])\n",
        "    p.add_tools(hover)\n",
        "\n",
        "    # Plot points with colors by group\n",
        "    p.scatter(\"x\", \"y\", source=source, fill_color=\"colors\", line_color=\"black\", size=15)\n",
        "\n",
        "    # Add labels to the points\n",
        "    p.text(\n",
        "        x=\"text_x\",\n",
        "        y=\"text_y\",\n",
        "        text=\"character\",\n",
        "        source=source,\n",
        "        text_font_size=\"8pt\",\n",
        "        text_baseline=\"middle\",\n",
        "        text_align=\"center\",\n",
        "    )\n",
        "\n",
        "    show(p)\n",
        "\n",
        "\n",
        "visualize_characters(emb_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36e3dad",
      "metadata": {
        "id": "f36e3dad"
      },
      "source": [
        "## Task 3: Generate word2vec embeddings\n",
        "\n",
        "Now, let us use word2vec to generate the embeddings. You can use the `gensim` library to generate the embeddings.\n",
        "\n",
        "- The input is the same as the previous task.\n",
        "- The output is the embedding matrix and the mapping from word to index, as is the case in the previous task.\n",
        "- Use `gensim.models.Word2Vec` to generate the embeddings.\n",
        "- When generating the word2vec embeddiing, make sure to train it using the skip-gram model with negative sampling by setting `sg = 1` and `hs = 0`.\n",
        "- Set min_count to 1, epochs to 10, window to 30 and vector_size to 32. Read the documentation of `gensim.models.Word2Vec` to understand the parameters for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73507b5c",
      "metadata": {
        "tags": [
          "gradable"
        ],
        "id": "73507b5c"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement this function\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "def generate_word2vec_embeddings(docs):\n",
        "    \"\"\"Generate word2vec embeddings from documents.\"\"\"\n",
        "    sequences = [doc.split() for doc in docs]\n",
        "    model = Word2Vec(sentences=sequences, vector_size=32, window=5, min_count=1, workers=2)\n",
        "\n",
        "    emb = np.zeros((len(model.wv.index_to_key), 32))\n",
        "    word_to_idx = dict(zip(model.wv.index_to_key, np.arange(len(model.wv.index_to_key))))\n",
        "\n",
        "    for i, word in enumerate(model.wv.index_to_key):\n",
        "        emb[i, :] = model.wv[word]\n",
        "\n",
        "    return emb, word_to_idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a23af8",
      "metadata": {
        "id": "14a23af8"
      },
      "source": [
        "Let us now generate the word2vec embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2416e29",
      "metadata": {
        "id": "c2416e29"
      },
      "outputs": [],
      "source": [
        "emb, word_to_idx = generate_word2vec_embeddings(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d93061",
      "metadata": {
        "id": "a1d93061"
      },
      "outputs": [],
      "source": [
        "assert emb.shape[1] == 32, \"The embedding dimension is not 32\"\n",
        "assert emb.shape[0] == len(word_to_idx), \"The number of words is not correct\"\n",
        "\n",
        "les_miserables_characters = df_characters[\"character\"].tolist()\n",
        "emb_characters = np.zeros((len(les_miserables_characters), emb.shape[1]))\n",
        "for i, character in enumerate(les_miserables_characters):\n",
        "    emb_characters[i, :] = emb[word_to_idx[character], :]\n",
        "\n",
        "focal_groups = [\"revolutionary_core\", \"young_paris_circle\", \"thenardier_gang\"]\n",
        "n_emb_characters = np.einsum(\n",
        "    \"ij,i->ij\", emb_characters, 1.0 / np.linalg.norm(emb_characters, axis=1)\n",
        ")\n",
        "group_centroids = np.array(\n",
        "    [\n",
        "        np.mean(n_emb_characters[df_characters[\"group\"] == group], axis=0)\n",
        "        for group in focal_groups\n",
        "    ]\n",
        ")\n",
        "group_similarieis = group_centroids @ group_centroids.T\n",
        "within_group_similarities = np.mean(np.diag(group_similarieis))\n",
        "between_group_similarities = np.sum(group_similarieis - np.diag(group_similarieis)) / (\n",
        "    len(focal_groups) * (len(focal_groups) - 1)\n",
        ")\n",
        "\n",
        "assert (\n",
        "    within_group_similarities > between_group_similarities\n",
        "), \"The groups are not well separated\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881d4fa1",
      "metadata": {
        "id": "881d4fa1"
      },
      "source": [
        "Let's visualize the word2vec embeddings. If successful, the groups should be well separated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728db1d5",
      "metadata": {
        "id": "728db1d5"
      },
      "outputs": [],
      "source": [
        "output_notebook()\n",
        "visualize_characters(emb_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c36e2936",
      "metadata": {
        "id": "c36e2936"
      },
      "source": [
        "Finally, confirm the most similar words to \"jean\" are \"javert\" and \"cosette\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e68c0a",
      "metadata": {
        "id": "c9e68c0a"
      },
      "outputs": [],
      "source": [
        "kv = gensim.models.KeyedVectors(emb.shape[1])\n",
        "kv.add_vectors([word for word in word_to_idx.keys()], emb)\n",
        "kv.most_similar(\"jean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee5bdf08",
      "metadata": {
        "id": "ee5bdf08"
      },
      "outputs": [],
      "source": [
        "kv.most_similar(\"javert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c609007",
      "metadata": {
        "id": "4c609007"
      },
      "outputs": [],
      "source": [
        "kv.most_similar(\"cosette\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc02078",
      "metadata": {
        "id": "1cc02078"
      },
      "source": [
        "## Task 4: Semaxis analysis\n",
        "\n",
        "Semaxis is a method to analyze the semantic space ([reference](https://aclanthology.org/P18-1228/)). Let us calculate the semaxis between \"justice\" and \"mercy\" and project the characters onto the semaxis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5ef390",
      "metadata": {
        "tags": [
          "gradable"
        ],
        "id": "3c5ef390"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement this function\n",
        "def get_semaxis(emb, kv, pos_word, neg_word):\n",
        "    \"\"\"Calculate the semantic axis between two words.\"\"\"\n",
        "    pos_vec = kv.get_vector(pos_word)\n",
        "    neg_vec = kv.get_vector(neg_word)\n",
        "\n",
        "    axis = neg_vec - pos_vec  # NOTE: order matters here!\n",
        "    axis = axis / np.linalg.norm(axis)  # normalize the axis\n",
        "\n",
        "    semaxis_scores = np.dot(emb, axis)  # project each character onto the axis\n",
        "\n",
        "    return semaxis_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35af510c",
      "metadata": {
        "id": "35af510c"
      },
      "outputs": [],
      "source": [
        "semaxis_scores = get_semaxis(emb, kv, \"justice\", \"mercy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b2293f",
      "metadata": {
        "id": "80b2293f"
      },
      "outputs": [],
      "source": [
        "# If the function is implemented correctly, the following test should pass.\n",
        "for i in range(10):\n",
        "    pos_word = np.random.choice(kv.index_to_key)\n",
        "    neg_word = np.random.choice(kv.index_to_key)\n",
        "    pos_vec = kv.get_vector(pos_word).copy()\n",
        "    neg_vec = kv.get_vector(neg_word).copy()\n",
        "    _semaxis_scores = get_semaxis(\n",
        "        np.array([[pos_vec - neg_vec], [neg_vec - pos_vec]]).reshape(2, -1),\n",
        "        kv,\n",
        "        pos_word,\n",
        "        neg_word,\n",
        "    )\n",
        "\n",
        "    assert np.allclose(\n",
        "        _semaxis_scores[0], 1\n",
        "    ), \"The semaxis scores are not correctly computed\"\n",
        "    assert np.allclose(\n",
        "        _semaxis_scores[1], -1\n",
        "    ), \"The semaxis scores are not correctly computed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81ad2c2",
      "metadata": {
        "id": "c81ad2c2"
      },
      "outputs": [],
      "source": [
        "# semaxis_scores = get_semaxis(emb, 'justice', 'mercy')\n",
        "semaxis_scores_characters = np.array(\n",
        "    [semaxis_scores[word_to_idx[character]] for character in les_miserables_characters]\n",
        ")\n",
        "\n",
        "# Plot the scores for the characters\n",
        "fix, ax = plt.subplots(figsize=(10, 10))\n",
        "order = np.argsort(semaxis_scores_characters)\n",
        "ax = sns.pointplot(\n",
        "    y=[les_miserables_characters[i] for i in order],\n",
        "    x=semaxis_scores_characters[order],\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_title(\"Semaxis scores for the characters\")\n",
        "ax.set_xlabel(\"Semaxis score: mercy (left) - law (right)\")\n",
        "ax.set_ylabel(\"Character\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "474187cd",
      "metadata": {
        "id": "474187cd"
      },
      "source": [
        "Let's also see the gender axis. Les Mis√©rables is a male-dominated story. But there are some female characters as well. Let's see how the characters are distributed on the gender axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340c077a",
      "metadata": {
        "id": "340c077a"
      },
      "outputs": [],
      "source": [
        "# This is the semaxis score for the gender axis.\n",
        "semaxis_scores = get_semaxis(emb, kv, \"man\", \"woman\")\n",
        "\n",
        "semaxis_scores_characters = np.array(\n",
        "    [semaxis_scores[word_to_idx[character]] for character in les_miserables_characters]\n",
        ")\n",
        "\n",
        "# Plot the scores for the characters\n",
        "fix, ax = plt.subplots(figsize=(10, 10))\n",
        "order = np.argsort(semaxis_scores_characters)\n",
        "ax = sns.pointplot(\n",
        "    y=[les_miserables_characters[i] for i in order],\n",
        "    x=semaxis_scores_characters[order],\n",
        "    hue=df_characters[\"gender\"][order],\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_title(\"Semaxis scores for the characters\")\n",
        "ax.set_xlabel(\"Semaxis score: woman (left) - man (right)\")\n",
        "ax.set_ylabel(\"Character\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808c7fa9",
      "metadata": {
        "id": "808c7fa9"
      },
      "source": [
        "Let's see the gender bias in occupations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b85dbc",
      "metadata": {
        "id": "d9b85dbc"
      },
      "outputs": [],
      "source": [
        "occupations = [\n",
        "    \"lawyer\",\n",
        "    \"doctor\",\n",
        "    \"teacher\",\n",
        "    \"nurse\",\n",
        "    \"cook\",\n",
        "    \"maid\",\n",
        "    \"servant\",\n",
        "    \"farmer\",\n",
        "    \"judge\",\n",
        "    \"police\",\n",
        "    \"engineer\",\n",
        "    \"writer\",\n",
        "    \"musician\",\n",
        "    \"merchant\",\n",
        "    \"baker\",\n",
        "    \"carpenter\",\n",
        "    \"painter\",\n",
        "    \"soldier\",\n",
        "    \"student\",\n",
        "]\n",
        "\n",
        "semaxis_scores = get_semaxis(emb, kv, \"man\", \"woman\")\n",
        "semaxis_scores_occupations = np.array(\n",
        "    [semaxis_scores[word_to_idx[occupation]] for occupation in occupations]\n",
        ")\n",
        "\n",
        "# Plot the scores for the characters\n",
        "fix, ax = plt.subplots(figsize=(10, 10))\n",
        "order = np.argsort(semaxis_scores_occupations)\n",
        "ax = sns.pointplot(\n",
        "    y=[occupations[i] for i in order], x=semaxis_scores_occupations[order], ax=ax\n",
        ")\n",
        "ax.set_title(\"Semaxis scores for the characters\")\n",
        "ax.set_xlabel(\"Semaxis score: woman (left) - man (right)\")\n",
        "ax.set_ylabel(\"Occupation\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1667dc",
      "metadata": {
        "id": "6d1667dc"
      },
      "source": [
        "Some occupations are more male-dominated than others. For example, \"judge\", \"lawyer\", \"writer\" and \"police\" are more male-dominated than others, despite the fact female characters may have these occupations.\n",
        "While such a gender bias is not explicitly present in the story, the word embedding captures implicit gender bias from the text.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388515b0",
      "metadata": {
        "id": "388515b0"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}